Project retrospective: AI Audiobook Voice Generator

Overview
- Goal: Build a local, simple-to-run web app that converts text to natural audiobook speech using a chosen narrator voice.
- Constraints: Single laptop GPU (RTX 3050 Ti, 4 GB VRAM), Windows environment, desire for a drop‑in model in the project folder, minimal user dependencies, and fast turnaround.
- Final outcome: Production-ready XTTS zero‑shot voice cloning integrated into Flask UI with chunking, progress feedback, and multiple voice options. Training custom models (VITS/Tacotron2) was explored deeply but ultimately abandoned based on evidence.

Starting point (before we paired)
- Local Windows project with Flask server and front-end (index.html), static assets, and an intent to enable multiple voices (VITS, custom narrator, future XTTS/TM button).
- Data: A growing audiobook dataset (initial 691 clips, later ~4,700 clips; total ~9–10 hours) with metadata lines like: clip_001.wav | text.
- Early wins: XTTS zero-shot had produced good quality “at one click,” but wasn’t yet cleanly integrated as a selectable voice in UI.
- Training direction: Interest to fine‑tune a model (initially VITS), confusion over “best_model” vs checkpoints, loss behavior, and whether we were truly fine‑tuning or training from scratch.

Phase 1 — VITS scratch attempt (unintended)
- Intent: Fine-tune VITS; reality: code initialized fresh model (Vits.init_from_config), not loading pre-trained weights.
- Symptoms: 18+ hours, ~50k steps, still no intelligible words; screeching reduced but no speech; epochs ~161/1000.
- Technical friction:
  - Windows multiprocessing errors (fixed by if __name__ == "__main__").
  - WinError 32 (log file locks) on cleanup.
  - CUDA OOM on 4 GB VRAM (fixed by smaller batch, fewer workers, env var: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True).
  - Tokenizer/config mismatches (phoneme settings, non-existent config fields).
- Post-mortem: We trained from scratch inadvertently; VITS needs much longer training, larger VRAM, and bigger, cleaner data to reach intelligibility.

Phase 2 — Correcting VITS path (attempted fine-tune)
- Plan: Load pre-trained VITS (e.g., LJSpeech) then fine‑tune.
- Status: Code adopted load_state_dict(strict=False) against pre-trained checkpoint.
- Symptoms: Training resumed from high global step (~1,000,080), but audio still garbage/screeching around +250–+1000 steps. Architecture/vocab/cleaner mismatches likely left key layers random; dataset/cleaner/tokenizer not aligned to source model.
- Conclusion: Even “fine-tuning” was effectively partial transfer; instability persisted. Risk of sinking time without gains was high.

Phase 3 — XTTS zero-shot integration (first production-ready path)
- Motivation: XTTS had already given strong results without training.
- Back-end update (app.py):
  - Added XTTS model load (tts_models/multilingual/multi-dataset/xtts_v2), checked reference clip (e.g., clip_001.wav), added /synthesize branch for "xtts".
  - Added chunking for long text, combined WAVs via pydub; console progress estimates for user feedback.
- Front-end update (index.html):
  - Added third voice button “TM” with provided image; pushed Convert button and audio player down; improved viewport and overflow CSS for consistent rendering on 127.0.0.1 vs LAN IP.
- Results: Stable, high‑quality audio; easy UX; zero training overhead.

Phase 4 — Tacotron2 training experiment (to learn and test a simpler model)
- Rationale: Seek a simpler, lighter model to train locally in hours.
- Dataset prep: Verified dataset.txt (filename|text), split into dataset_train.txt and dataset_eval.txt (90/10).
- Custom formatter: Implemented audiobook_formatter to load two-column metadata (LJSpeech expects three columns by default).
- Tokenizer fix: Initialized TTSTokenizer before model to avoid empty vocab errors.
- Training: Ran on full dataset; checkpoints saved; testing script generated samples.
- Symptoms at ~1,100 steps: Max decoder steps reached (10000), screeching/very short outputs, wildly inconsistent durations, some samples stuck for minutes. Classic non‑alignment / training failure behavior.
- Related known issues: Tacotron2 often fails to align with mismatched cleaners/tokenization, insufficient batch sizes, or noisier/longer clips; decoder may run to max steps when stop prediction/attention fails.
- Decision: Stop training—evidence pointed to low odds of success on 4 GB VRAM, dataset/domain specifics, and library sensitivity.

Phase 5 — Finalizing the recommended solution
- Outcome: Anchored on XTTS zero-shot voice cloning, with Flask UI, progress messages, and chunked synthesis for long passages.
- Checkpoint/testing utilities: Built scripts to evaluate model checkpoints historically (for VITS/Tacotron phases), but ultimately not needed for XTTS.
- Deployment principle: Keep users’ dependencies minimal; prefer cached model in local folder with setup script when needed; otherwise use TTS library to manage model assets.

What we tried that was wrong or suboptimal
- VITS defaults that built a model from scratch (not fine-tune), consuming hours without speech.
- Overly optimistic assumptions about steps-to-intelligibility on small VRAM and mixed dataset without robust alignment tooling.
- Tacotron2 without layer-freeze/warm starts/pretrained alignments; relying on raw character pipeline and large heterogeneous audiobook segments; insufficient batch or curriculum for alignment.

Why training failed (core reasons)
- Compute: 4 GB VRAM restricts effective batch sizes, slows steps, and increases instability.
- Data: Audiobook narration has variable prosody/phrasing; even at 10 hours, alignment can be brittle without strict preprocessing (e.g., trimming silences, consistent loudness, strict text normalization/cleaners, phonemes pipeline).
- Library sensitivity: Version frictions (deprecated autocast, tokenizer/config fields), Windows file locks, and multiprocessing semantics compounded debugging cost.
- Transfer mismatch (VITS fine-tune): Pretrained charsets/cleaners/architectural expectations differ; partial load leaves critical submodules random; instability persists.
- Alignment: Tacotron2 requires early alignment formation; when attention/stopnet fails, decoder runs to max steps and produces screeching or empty output.

What the 1% of 1% do differently
- Data engineering: Studio-grade recordings; flawless transcriptions; controlled prompt scripts for phoneme coverage; silence trimming and loudness normalization at scale; phonemization with robust cleaners.
- Training pipeline: Pretrained checkpoints matched in tokenizer/cleaners; layer freezing, gradual unfreezing; curriculum (short→long utterances); large batches; scheduled LR; alignment diagnostics and auto‑stop if no alignment by N steps; separate vocoder training or robust neural vocoder selection.
- Infrastructure: Multi‑GPU (A100 class, 40–80 GB), distributed training; weeks of continuous runs; automated evaluation; MLOps to track regressions; budget for experimentation.
- Economics: Often choose zero-shot/few-shot systems or commercial services unless differentiation justifies custom training cost.

Key takeaways you can reuse
- Choose the right tool: Zero‑shot XTTS is purpose‑built for cloning; it won quickly.
- If training anyway: Start from matched pretrained checkpoints; freeze most layers; align tokenizer/cleaners; phonemes often help; watch alignment early (stop if absent by ~1–2k steps); use curriculum; keep batches as large as VRAM allows; trim silences and normalize loudness.
- Engineering the UX: Chunk long text, show progress estimates, handle errors clearly, and cache models for predictable user experience.

What we shipped
- Flask app with three voices (VITS demo, XTTS “TM”, “Tatiana” slot).
- XTTS path: zero-shot cloning with your reference clip, chunked synthesis, stable and high quality.
- Front-end improvements: Consistent viewport scaling, overflow fixes, accessible voice selection, and clean layout.

Where to go next (if you revisit training later)
- Try GlowTTS/AlignTTS with phonemes and strict cleaners; curriculum on short utterances first.
- Use pretrained checkpoints that exactly match tokenizer/cleaners (and freeze/unfreeze schedule).
- Consider small‑scale LoRA/adapters on a strong pretrained backbone rather than full-model finetune.
- Increase VRAM (cloud A10/A100 for short sprints) and budget 2–3 days for iterative trials.

Quick sanity checklist (for any future training)
- Data:
  - Text strictly matches speech; normalize punctuation; filter long/very short clips; trim silences; RMS/loudness normalize.
- Config:
  - Tokenizer/cleaners/phoneme settings match pretrained model; correct sample rate.
- Training:
  - Freeze encoder first; small LR; watch alignment by ~1k–2k steps; save often; test checkpoints early.
- Infra:
  - Stable CUDA/driver/version set; disable excess workers on Windows; set safe batch sizes.

Bottom line
- We explored broadly and deeply; we validated limits under local constraints.
- The production solution (XTTS zero‑shot) meets quality and simplicity goals reliably.
- The failed training attempts were valuable: you now own the practical heuristics to evaluate, design, and debug TTS trainings—and to decide when not to.

================================================================================================

Technical report: AI Audiobook Voice Generator

Overview
- Objective: Local web app to convert user text to narrated audio with selectable voices. Final production path uses XTTS zero-shot voice cloning via Flask API with a simple front end, long-text chunking, progress logging, and cached model assets.
- Platform: Windows 11, Python 3.11, single GPU (RTX 3050 Ti 4GB VRAM), virtualenv.
- Core stacks: Flask (server), HTML/CSS/JS (UI), Coqui TTS (models and synthesis), PyTorch/CUDA (GPU runtime), pydub/ffmpeg (concatenation), glob/os/pathlib (I/O), time/logging (progress), fsspec (trainer utils).

Project file structure (representative)
D:\Programming\Projects\AI AudioBook Voice Generator\Working file\
- app.py
  - Flask server. Exposes / and /synthesize endpoints. Loads three voices: VITS (pretrained), XTTS (zero-shot cloning using reference WAV), and Tatiana (custom trained model if available). Implements long-text chunking, progress logging, and output file management.
- templates\index.html
  - Front-end page. Three circular voice buttons (VITS, TM/XTTS, Tatiana), text box, Convert button, audio player. Viewport/overflow CSS fixes.
- static\
  - logo.jpg, TM.png, vits1.jpg, xtts_cloned.png, photo1.png (assets)
  - audio_*.wav (generated files served by Flask)
- AudioBook\HGclips\
  - clip_XXXX.wav (4115 clips)
  - dataset.txt (source metadata: filename | text)
  - dataset_train.txt, dataset_eval.txt (90/10 split for experiments)
- model_output\
  - tacotron2-YYYYMMDD_HHMMSS\ (Tacotron 2 experimental runs: config.json, checkpoint_*.pth, best_model.pth)
  - other run folders (VITS experiments; not used in final prod path)
- Testing audio files\
  - Tacotron2 Checkpoint Output\ (generated test WAVs from checkpoint tester)
- Scripts (used during R&D; not all required for production)
  - train_tacotron2.py (Tacotron 2 training attempt using custom formatter and tokenizer init)
  - prepare_tacotron_dataset.py (train/val split for dataset.txt)
  - check_dataset.py (data presence/format sanity checks)
  - test_tacotron2_checkpoints.py (batch checkpoint evaluator)
  - test_checkpoints.py (earlier generic evaluator; path-updated per run)

Endpoint and runtime flow (final XTTS path)
1) GET / -> serves index.html.
2) UI: user selects voice (vits | xtts | tatiana), enters text, clicks Convert.
3) POST /synthesize { text, voice }.
   - Cleans old generated WAVs (older than threshold) to keep static/ tidy.
   - For long text: splits into chunks (paragraph/sentence-aware), generates chunk WAVs sequentially, combines via pydub, deletes temps, returns final output path.
   - For XTTS: calls TTS API with model "tts_models/multilingual/multi-dataset/xtts_v2", speaker_wav reference clip, language="en", and file_path.
4) Responds JSON { file, duration, chunks }.
5) Front-end sets audioPlayer.src to file and plays.

Key modules and libraries
1) Flask
- What it is: A lightweight Python web framework for building HTTP APIs and serving HTML.
- Why used here: To expose synthesis as a local web service, render the UI, and serve generated audio.
- What it’s doing: 
  - app.py routes: index() returns index.html; synthesize() handles JSON body, dispatches to selected model, manages temporary files, logs progress, returns output file path.

2) Coqui TTS (coqui-ai/TTS)
- What it is: Open-source text-to-speech toolkit with multiple TTS architectures (XTTS, VITS, Tacotron2, FastSpeech, etc.) and utilities (Synthesizer, tokenizers, dataset loaders, training loops).
- Why used here: Provides production-grade XTTS zero-shot inference and experimental training backbones (VITS/Tacotron2) with unified APIs.
- What it’s doing:
  - Final solution: TTS("tts_models/multilingual/multi-dataset/xtts_v2").tts_to_file(text, speaker_wav, language, file_path) for zero-shot voice cloning.
  - Experiments: 
    - Synthesizer(tts_checkpoint, tts_config_path) to load checkpoints (VITS/Tacotron2) for evaluation.
    - Trainer/TrainerArgs for training loops.
    - Dataset utilities: load_tts_samples, custom formatter integration.
    - Tokenizer (TTSTokenizer) initialization to set character vocab for Tacotron2.

3) PyTorch (torch, torchaudio)
- What it is: Deep learning framework for GPU-accelerated tensor operations and model training/inference.
- Why used here: Backend runtime for TTS models; leverages CUDA GPU acceleration for fast synthesis and (experimental) training.
- What it’s doing: 
  - XTTS/VITS/Tacotron2 model weights and computation.
  - CUDA selection (to("cuda")) and autocast runtime (deprecation warnings noted but harmless).

4) CUDA and NVIDIA drivers
- What it is: NVIDIA GPU compute stack enabling PyTorch CUDA ops.
- Why used here: GPU acceleration reduces synthesis time for XTTS and was required to make any training feasible.
- What it’s doing: Kernel execution for matrix multiplications, convolutions; heavily used in vocoder/synthesis.

5) pydub (+ ffmpeg/ffprobe)
- What it is: High-level audio manipulation library wrapping ffmpeg.
- Why used here: To concatenate chunk WAVs into one final audio file when texts are long.
- What it’s doing: 
  - AudioSegment.from_wav(temp.wav) and combined.export(output.wav, format="wav").

6) ffmpeg / ffmpeg-python (runtime dependency for pydub)
- What it is: Industry-standard audio/video processing tool.
- Why used here: Required by pydub to read/merge/export WAVs reliably.
- What it’s doing: Backend transcoding/concatenation operations during chunk merge.

7) Standard library modules: os, glob, time, re, shutil, pathlib
- What they are: Python built-ins for filesystem, pattern matching, timing, regex, deletion, and paths.
- Why used here: Operational glue—path management, old-file cleanup, chunk creation, and logging.
- What they’re doing:
  - os.path join/exist/makedirs; glob to enumerate files; time for timestamps/ETAs; re for sentence/paragraph splitting; shutil for cleanup.

8) fsspec (via TTS trainer utils)
- What it is: Unified filesystem interface library.
- Why used here: Indirectly via trainer.generic_utils.remove_experiment_folder and fsspec local implementation during experimental runs.
- What it’s doing: Deleting experiment folders; in Windows, care for open-handle issues.

9) HTML/CSS/JavaScript (front end)
- What they are: UI stack for web page controls and interactions.
- Why used here: Provide a clean UI with three voice selectors, a text area, convert button, and audio player; ensure consistent rendering on localhost and LAN.
- What they’re doing:
  - index.html includes: three circular image buttons (VITS, TM/XTTS, Tatiana), Convert button separated into its own row, and an audio tag.
  - CSS fixes: viewport meta tag; overflow hidden; object-fit: cover; responsive widths; side-image constraints.
  - JS: Selected voice state, POST to /synthesize with {text, voice}, update UI while processing, autoplay audio on success; optional long-text warning for XTTS.

Final code responsibilities (core production files)
- app.py
  - Model loading: 
    - vits_tts = TTS("tts_models/en/ljspeech/vits", gpu=True) [demo voice].
    - xtts_tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to("cuda") [primary voice cloning].
    - tatiana_tts = Synthesizer(tts_checkpoint, tts_config_path, use_cuda=True) [optional custom slot].
  - Reference voice: NARRATOR_VOICE_CLIP (e.g., ...\clip_001.wav) validated on startup.
  - Synthesis: 
    - voice=="xtts": xtts_tts.tts_to_file(text/chunk, speaker_wav=NARRATOR_VOICE_CLIP, language="en", file_path=...)
    - voice=="vits": vits_tts.tts_to_file(text/chunk, file_path=...)
    - voice=="tatiana": wav = tatiana_tts.tts(text/chunk); tatiana_tts.save_wav(wav, ...)
  - Long-text chunking: split_text_into_chunks(text, max_chars≈1000) by paragraphs then sentences; merge WAV chunks with pydub.
  - Housekeeping: delete old static/audio_*.wav; guard for model/clip availability; detailed console progress (chunk indices, estimated time, final duration).

- templates\index.html
  - UI scaffold: header with logo/title; description; textarea; voice-selection buttons (VITS, TM/XTTS, Tatiana) with glowing selection class; Convert button (separate row); audio player below.
  - UX polish: viewport meta; overflow hidden; responsive widths; limited asset max-width; wrapped voice buttons on small screens.
  - Client logic: selectedVoice state; validation; POST /synthesize; disable/enable button around request; autoplay on success; optional long-text timing hint for XTTS.

Experimental/training files (learning artifacts, not used in final deployment)
- train_tacotron2.py
  - Purpose: Experiment with a simpler model than VITS. Includes custom audiobook_formatter for filename|text datasets, tokenizer initialization (TTSTokenizer) prior to model creation, and Tacotron2Config tuned for 4GB VRAM. Saved to model_output\tacotron2-... with checkpoints every N steps.
- prepare_tacotron_dataset.py
  - Purpose: Create dataset_train.txt and dataset_eval.txt from dataset.txt with a stable randomized split (seed=42) for Tacotron2 experiments.
- check_dataset.py
  - Purpose: Validate audio paths referenced by dataset.txt; print missing file counts; preview first lines.
- test_tacotron2_checkpoints.py
  - Purpose: Batch-load Tacotron2 checkpoints and synthesize several fixed prompts to audit progression; write outputs to Testing audio files\Tacotron2 Checkpoint Output.

Key technical decisions and their rationale
- XTTS zero-shot as primary synthesis path
  - Rationale: Works reliably on limited hardware; high quality without training; matches requirement to avoid heavy user-side installs (only TTS/PyTorch on server). Allows storing model cache locally for offline use after first download.
- Long-text chunking + concatenation
  - Rationale: Prevents model stalls and extreme generation times; reduces risk of memory/timeouts; yields a seamless final WAV for playback/download.
- Progress logging (server-console) and UI prompts
  - Rationale: XTTS is silent during long synth; visible progress estimates reduce perceived latency.
- CSS viewport and overflow fixes
  - Rationale: Ensure identical rendering on localhost vs LAN IP; eliminate unexpected scroll.

Operational notes
- Model asset caching: First run of XTTS downloads ~GBs into user cache (e.g., %USERPROFILE%\.local\share\tts\...). Optional setup script can copy to project models/ directory.
- Dependencies to install on the host running the server: TTS, torch+CUDA, flask, pydub, ffmpeg present in PATH (or ffmpeg-python). Users of the web UI do not need local ML installs; only the server machine needs them.
- Windows specifics: Use if __name__ == "__main__" for any training/multiprocessing scripts; avoid large num_loader_workers; close file explorers watching run directories to prevent WinError 32 during deletions.

Quick library summary mapping
- Flask: web server; routes UI and synthesis API; manages generated files.
- Coqui TTS: XTTS model loading and synthesis; experimental backbones (VITS/Tacotron2) for R&D; checkpoint evaluation.
- PyTorch/CUDA: core tensor engine on GPU for all models.
- pydub + ffmpeg: combine chunked WAVs into final audio.
- Standard lib (os/glob/pathlib/time/re): file ops, discovery, chunk logic, timestamps, console feedback.
- fsspec (indirect): remove and manage experiment folders safely (training utilities).

What to keep for maintenance
- Confirm NARRATOR_VOICE_CLIP path exists at startup; handle missing case gracefully.
- Keep chunk size around ~800–1200 characters per chunk for stable XTTS runtimes; adjust based on typical latency.
- Maintain pydub + ffmpeg availability; without ffmpeg pydub export will fail.
- Monitor static/ for buildup; keep cleanup threshold (e.g., delete outputs older than 1 hour) to avoid clutter.
- Pin TTS and torch versions in requirements.txt to avoid breaking changes.

Requirements baseline (server)
- Python 3.11
- torch + matching CUDA toolkit for your GPU
- TTS
- flask
- pydub
- ffmpeg accessible in PATH (or install ffmpeg-python but system ffmpeg is recommended)

This technical description captures the production architecture (XTTS + Flask + chunk/merge), the supporting file structure, and the roles of each library and script used across both the final solution and the R&D phases.


====================================================================================================

Here’s a concise report documenting the “Custom upload voice” update and the UX/tech fixes applied.

Overview
- Goal: Enable a third “Custom” button to upload a 10–30s sample voice (any author), use it as XTTS reference, generate audio, then auto-delete the uploaded clip.
- Result: Frontend shows clear upload status, supports many audio types, remembers selection, and backend accepts/uploads, auto-converts to WAV, uses it for XTTS cloning, and cleans up.

Frontend changes
- Added “Custom” button with circular image and label positioned consistently below the circle for all three buttons.
- Improved file input accept filter to show audio files reliably:
  - input accept="audio/*,.wav,.mp3,.m4a,.flac,.ogg,.aac,.wma, .webm,.opus" so browsers reveal typical audio formats.[1][2][3]
- Added visible upload status banner with three states:
  - uploading (yellow), success (green), error (red).
- Fixed selection logic:
  - After successful upload, programmatically selects the Custom button via selectVoice('custom').
  - Debug logs added to verify state transitions in DevTools Console.
- Prevented “Please select a voice” error:
  - Ensured selectedVoice is set and the .selected class applied on upload completion.

Backend changes
- Upload route /upload_voice:
  - Accepts many extensions: wav, mp3, m4a, flac, ogg, aac, wma, webm, opus.
  - Uses secure_filename, stores to Sample_Clip with timestamp.
  - Auto-converts any uploaded format to 22,050 Hz WAV via pydub/ffmpeg to standardize XTTS input, improving consistency and avoiding codec edge-cases.
  - Returns normalized filename (custom_voice_<ts>.wav).
  - Before each upload and after synthesis, deletes all Sample_Clip files to avoid accumulation.
- Synthesis route /synthesize:
  - When voice="custom", resolves Sample_Clip/custom_voice_<ts>.wav and runs XTTS with speaker_wav set to that path.
  - Chunking for long text remains, combining chunks via pydub.
  - After generating output, deletes the uploaded sample.

Why these fixes matter
- Accept and format support:
  - HTML accept is a hint, not enforcement; some browsers still hide files if patterns are too narrow—using audio/* and explicit extensions improves discoverability.[2][3][1]
- Auto-conversion to WAV:
  - XTTS generally handles most formats, but WAV at a standard sample rate is most robust and avoids codec-specific failures; pydub+ffmpeg performs this reliably.
- UX feedback:
  - Clear upload status reduces user confusion, confirms readiness, and ensures state (custom voice selected) is visible before conversion.

Recommended user guidance (UI copy)
- “Upload a clear 10–30 second narration clip without music or noise for best cloning” (XTTS works with short clips, but quality improves with clean speech).[4][5][6]
- “Supported formats: WAV, MP3, M4A, FLAC, OGG, AAC, WMA, WEBM, OPUS; files auto-convert to WAV.”

Key components and roles
- Frontend:
  - Custom button with image and label below circle.
  - File input accept filter for broad audio support.[1][2]
  - Upload status banners and enforced selection of “Custom” after upload.
- Backend:
  - /upload_voice: validate → save → convert to 22,050 Hz WAV → return filename.
  - /synthesize: resolves voice selection (vits | xtts | custom), runs XTTS with correct speaker_wav, chunks long text, cleans up Sample_Clip folder post-generation.

Operational notes
- ffmpeg must be installed and on PATH for pydub conversions to work.
- Auto-cleanup removes all prior samples; if multi-user is planned, store per-session filenames and only remove that file post-use.
- For even better accuracy, prompt the user to upload a clip with:
  - Solo speech, minimal room reverb, no background music, clear articulation, and stable volume.

References
- XTTS model docs and usage patterns indicating short reference clips and WAV outputs.[5][6][7][4]
- HTML file input accept behavior and best practices.[3][2][1]
- Flask file upload patterns for reliable server-side validation.[8][9]
